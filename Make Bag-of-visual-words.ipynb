{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Bag-of-visual-words\n",
    "\n",
    "* Load Image's descriptor for all images. \n",
    "* Find corresponding visual word index for each descriptor. \n",
    "* Save list of visual word index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Oxford 5k dataset, You can use already provided visual words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_descriptor: 16334970\n"
     ]
    }
   ],
   "source": [
    "# Oxford 5k dataset provides already converted visual words. We could use this one\n",
    "import os \n",
    "import pickle\n",
    "\n",
    "oxf5k_visualword_dir = './data/word_oxc1_hesaff_sift_16M_1M'\n",
    "\n",
    "filelist = os.listdir(oxf5k_visualword_dir)\n",
    "filelist.sort()\n",
    "# print(filelist)\n",
    "# for parent_dir, _, files in os.walk(oxf5k_visualword_dir):\n",
    "#     print(files)\n",
    "\n",
    "bow_dict = {}\n",
    "count_descriptor = 0\n",
    "image_feature_count_info = []\n",
    "for filename in filelist:\n",
    "    filepath = os.path.join(oxf5k_visualword_dir, filename)\n",
    "    image_name = filename.replace(\".txt\", \"\")    \n",
    "    visual_words = []\n",
    "    with open(filepath) as f:\n",
    "        lines = list(map(lambda x: x.strip(), f.readlines()[2:])) # ignore first two lines        \n",
    "        for l in lines:\n",
    "            val = l.split(\" \")\n",
    "            visual_word_index = int(val[0])-1 # This data use 1 to 1,000,000. convert to zero-based so 0 to 999,999  \n",
    "            visual_words.append(visual_word_index)\n",
    "            # print('{} descriptor {}'.format(filename, l))\n",
    "        count_descriptor = count_descriptor + len(lines)\n",
    "    image_feature_count_info.append((image_name, len(visual_words)))\n",
    "    bow_dict[image_name] = sorted(visual_words)\n",
    "    # break\n",
    "# print('bow_dict:', bow_dict)\n",
    "print('count_descriptor:', count_descriptor)\n",
    "\n",
    "\n",
    "pickle.dump(bow_dict, open('bow_dict_word_oxc1_hesaff_sift_16M_1M_pretrained.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Or we could make a new one\n",
    "\n",
    "## Requirements\n",
    "\n",
    "* Image name and its associated 128d descriptors\n",
    "* visual words assigner\n",
    "    * For a given 128d descriptor, it tells index of visual words. \n",
    "    * After you ran k-means clustering, you could use it to get nearest centroid's id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For oxford 5k dataset, it may be possible to recover the \"Image name and its associated 128d descriptors\"\n",
    "from utils.oxf5k_feature_reader import feature_reader\n",
    "\n",
    "# Frist, read all 128d descriptor of image.\n",
    "feature_bin_path = \"./data/feature/feat_oxc1_hesaff_sift.bin\"\n",
    "all_features = feature_reader(feature_bin_path)\n",
    "print('num of 128d descriptors in oxf5k: ', len(all_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the order of filenames related to above features. Refer README in http://www.robots.ox.ac.uk/~vgg/data/oxbuildings/README2.txt\n",
    "feature_bin_filename_order = \"./data/feature/order.txt\"\n",
    "with open(feature_bin_filename_order) as f:\n",
    "    filenames = list(map(lambda x: x.strip(), f.readlines()))\n",
    "\n",
    "# Check compatibility of name order\n",
    "for idx, val in enumerate(image_feature_count_info):\n",
    "    name_from_vw, _ = val\n",
    "    if name_from_vw == filenames[idx]:\n",
    "        continue\n",
    "    else:\n",
    "        print(idx, name_from_vw, filenames[idx])\n",
    "print(\"compatibility check done\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "image_descriptor_dict = {} # key: image_name, value: 2d numpy array of shape (num_descriptor, dim_descriptor)\n",
    "start_idx = 0\n",
    "for image_name, num_descriptor in image_feature_count_info:\n",
    "    val = np.array(all_features[start_idx:(start_idx+num_descriptor)], dtype=np.uint8)\n",
    "    image_descriptor_dict[image_name] = val\n",
    "    \n",
    "    start_idx += num_descriptor\n",
    "    # break\n",
    "# print('image_descriptor_dict:', image_descriptor_dict)\n",
    "with open('image_descriptor_dict_oxc1_hesaff_sift_16M.pkl', 'wb') as f:\n",
    "    pickle.dump(image_descriptor_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5062\n"
     ]
    }
   ],
   "source": [
    "# Load image descriptor dictionary, and assign each descriptor to visual words\n",
    "import pickle\n",
    "with open('image_descriptor_dict_oxc1_hesaff_sift_16M.pkl', 'rb') as f:\n",
    "    # key: image_name, value: 2d numpy array of shape (num_descriptor, dim_descriptor)\n",
    "    image_descriptor_dict = pickle.load(f) \n",
    "print(len(image_descriptor_dict))\n",
    "\n",
    "with open('kmeans.pkl', 'rb') as f:\n",
    "    kmeans = pickle.load(f)\n",
    "    \n",
    "# For PQ-kmeans clustering, we first convert query to PQ codes. \n",
    "with open('encoder.pkl', 'rb') as f:\n",
    "    encoder = pickle.load(f)\n",
    "\n",
    "    \n",
    "bow_dict = {}\n",
    "for image_name in image_descriptor_dict.keys():\n",
    "    descriptors = image_descriptor_dict[image_name]    \n",
    "    data_points_pqcode = encoder.transform(descriptors)\n",
    "    visual_words = kmeans.fit_predict(data_points_pqcode)\n",
    "    bow_dict[image_name] = sorted(visual_words)\n",
    "    sorted(clustered)\n",
    "    print(clustered)\n",
    "    break\n",
    "pickle.dump(bow_dict, open('bow_dict_word_oxc1_hesaff_sift_16M_1M_handmade.pkl', 'wb'))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1.3_global",
   "language": "python",
   "name": "tf1.3_global"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
