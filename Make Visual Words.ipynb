{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Visual Words\n",
    "\n",
    "## Rquirements\n",
    "\n",
    "k means clustering is memory intensive process. \n",
    "\n",
    "## Input\n",
    "\n",
    "* n-dim descriptors\n",
    "    * For making visual words, which image contains which descriptor is not important. We just do quantization.\n",
    "\n",
    "* Number of visual words\n",
    "\n",
    "## Process\n",
    "\n",
    "Build cluster.\n",
    "Say, we use k-means clustering. We determine number of clusters. \n",
    "Run k-means clustering. \n",
    "We can accelerate speed by using approximate nearest neighbor search such as kd-tree, but we lose accuracy. \n",
    "\n",
    "## Output\n",
    "\n",
    "index of visual words, and its cluster's center point. \n",
    "\n",
    "From a given feature, we can use the output information to determine most closest visual words. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "16334970\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load all features in the dataset. \n",
    "# Q. What if there is excatly same feature many times? do we add it into feature pool? \n",
    "# Q. Does many same feature point affect the result of k-menas clustering?\n",
    "\n",
    "# An image may contain several features, and we need to get all features. \n",
    "\n",
    "\n",
    "% load_ext autoreload\n",
    "% autoreload 2\n",
    "# from utils import oxf5k_feature_reader as feature_reader\n",
    "\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "def feature_reader(feature_bin_path=\"./data/feature/feat_oxc1_hesaff_sift.bin\"):\n",
    "    \"\"\"\n",
    "    This method reads official oxf5k descriptor.\n",
    "    \n",
    "    binary file contains 128byte sift descriptor. It is 128-d integer vector. \n",
    "    \"\"\"\n",
    "        \n",
    "    features = []    \n",
    "    # Read feature from bin file, and make tuple format. \n",
    "    with open(feature_bin_path, \"rb\") as f:\n",
    "        # Read 128 byte        \n",
    "        raw_binary = f.read(128)\n",
    "        while len(raw_binary) == 128:\n",
    "            dt = np.dtype(np.uint8)\n",
    "            # dt = dt.newbyteorder('>')\n",
    "            descriptor = np.frombuffer(raw_binary, dtype=dt)\n",
    "            features.append(descriptor)\n",
    "            raw_binary = f.read(128)\n",
    "            \n",
    "    return features\n",
    "\n",
    "\n",
    "def load_features_from_dataset():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "all_features = feature_reader()\n",
    "print(len(all_features))\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "data_points: [[ 38  18   4 ...,  45  49  10]\n",
      " [110  50   4 ...,   4  44  43]\n",
      " [102  28   1 ...,  50 110   2]\n",
      " ..., \n",
      " [ 65  83  58 ...,   0   0   3]\n",
      " [ 34  64  70 ...,   1   1  13]\n",
      " [ 22   8  36 ...,  27  12  13]]\n",
      "data_points shape: (16334970, 128)\n",
      "data_points dtype: uint8\n",
      "k: 131072\n",
      "max_val: 255\n",
      "center_points: [[ 27 230  37 ...,  65  11  53]\n",
      " [157  20  91 ...,  97  48 215]\n",
      " [245  69   0 ..., 165 112  25]\n",
      " ..., \n",
      " [163 177 152 ...,   0 193 188]\n",
      " [249 186 241 ..., 207 174 204]\n",
      " [ 39 228 217 ..., 113 166 245]]\n",
      "center_points type: uint8\n"
     ]
    }
   ],
   "source": [
    "% load_ext autoreload\n",
    "% autoreload 2\n",
    "from utils import clustering_utils\n",
    "import numpy as np\n",
    "\n",
    "dim = 128\n",
    "\n",
    "# n = 80000\n",
    "# k = 20000\n",
    "# data_points = np.random.randint(256, size=(n, dim), dtype=np.uint8)\n",
    "\n",
    "data_points = np.array(all_features)\n",
    "k = 131072 # 2^17 = 131072\n",
    "\n",
    "# Timing History:\n",
    "# n = 80,000 k=20,000 => single k-d tree with FLANN, 13 step, 6 min \n",
    "\n",
    "\n",
    "print('data_points:', data_points)\n",
    "print('data_points shape:', data_points.shape)\n",
    "print('data_points dtype:', data_points.dtype)\n",
    "print('k:', k)\n",
    "\n",
    "max_val = 2**8-1 # 255. uint8 max. for SIFT descriptor\n",
    "print('max_val:', max_val)\n",
    "center_points = np.random.randint(max_val, size=(k, dim), dtype=np.uint8) \n",
    "# center_points = center_points.astype(float)\n",
    "print('center_points:', center_points)\n",
    "print('center_points type:', center_points.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encode with subset\n",
      "shape of rand sample: (100000, 128)\n",
      "CPU times: user 2min 15s, sys: 1h 54min 12s, total: 1h 56min 28s\n",
      "Wall time: 2min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import random\n",
    "import pqkmeans\n",
    "import pickle\n",
    "\n",
    "# Train a PQ encoder.\n",
    "# Each vector is devided into 4 parts and each part is\n",
    "# encoded with log256 = 8 bit, resulting in a 32 bit PQ code.\n",
    "print('encode with subset')\n",
    "M=4\n",
    "encoder = pqkmeans.encoder.PQEncoder(num_subdim=M, Ks=256)\n",
    "# Q. do we have to use only subset? If we have train/test set split, use train for this. use test for query\n",
    "# encoder.fit(data_points[:1000])\n",
    "# time complexity for PQ: O(DL) where D is number of points, L is ???\n",
    "\n",
    "num_to_select = 100000                     # set the number to select here. 100K for 16M\n",
    "#  1000000\n",
    "# 16000000\n",
    "selected_index = np.random.choice(data_points.shape[0], num_to_select, replace=False)\n",
    "list_of_random_items = data_points[selected_index, :]\n",
    "print('shape of rand sample:', list_of_random_items.shape)\n",
    "\n",
    "encoder.fit(list_of_random_items)  # Use top 1M descriptor for training visual words codebook for oxford5k\n",
    "pickle.dump(encoder, open('pqencoder_100k_random_sample_from_16M.pkl', 'wb'))\n",
    "\n",
    "# Timing. codebook learning of 128d 100k took 3 min\n",
    "# Timing. codebook learning of 128d 1M took 30 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/16334970 [00:00<885:22:42,  5.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform whole set\n",
      "data_points_pqcode.shape:\n",
      "(16334970, 4)\n",
      "data_points_pqcode.nbytes:\n",
      "65339880 bytes\n",
      "data_points_pqcode.dtype:\n",
      "uint8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16334970/16334970 [02:50<00:00, 96020.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21min 20s, sys: 1h 31min 59s, total: 1h 53min 20s\n",
      "Wall time: 2min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('transform whole set')\n",
    "# Convert input vectors to 32-bit PQ codes, where each PQ code consists of four uint8.\n",
    "# You can train the encoder and transform the input vectors to PQ codes preliminary.\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "# # For big-data that cannot fit in memory, we can use generator\n",
    "pqcode_generator = encoder.transform_generator(data_points)\n",
    "N, _ = data_points.shape\n",
    "data_points_pqcode = np.empty([N, M], dtype=encoder.code_dtype)\n",
    "print(\"data_points_pqcode.shape:\\n{}\".format(data_points_pqcode.shape))\n",
    "print(\"data_points_pqcode.nbytes:\\n{} bytes\".format(data_points_pqcode.nbytes))\n",
    "print(\"data_points_pqcode.dtype:\\n{}\".format(data_points_pqcode.dtype))\n",
    "for n, code in enumerate(tqdm(pqcode_generator, total=N)):\n",
    "    data_points_pqcode[n, :] = code\n",
    "\n",
    "# For small data fit in memory, simply run this. \n",
    "# data_points_pqcode = encoder.transform(data_points)\n",
    "\n",
    "np.save('data_points_pqcode_with_100k_codebook.npy', data_points_pqcode)\n",
    "\n",
    "# Memory: With 32bit PQ code, 16M 128d descriptor takes 65339960 bytes = 62.3 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run k-means with k: 131072\n",
      "clustered len: 16334970\n",
      "CPU times: user 3d 15h 37min 25s, sys: 5min 32s, total: 3d 15h 42min 57s\n",
      "Wall time: 2h 13min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('run k-means with k:', k)\n",
    "# Run clustering with k=5 clusters.\n",
    "kmeans = pqkmeans.clustering.PQKMeans(encoder=encoder, k=k)\n",
    "clustered = kmeans.fit_predict(data_points_pqcode)\n",
    "\n",
    "print('clustered len:', len(clustered))\n",
    "\n",
    "clustering_centers_numpy = np.array(kmeans.cluster_centers_, dtype=encoder.code_dtype)  # Convert to np.array with the proper dtype\n",
    "np.save('clustering_centers_numpy.npy', clustering_centers_numpy)# Then, clustered[0] is the id of assigned center for the first input PQ code (X_pqcode[0])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16334970\n",
      "[98066, 86805, 123344, 56272, 78779, 126036, 46672, 10683, 71195, 6844, 104562, 90043, 59374, 117617, 14484, 2925, 98530, 27423, 122756, 25932, 46399, 112124, 35213, 1630, 120951, 87843, 3350, 5630, 2156, 128053, 38998, 84877, 124697, 70982, 63487, 55803, 2421, 107937, 41971, 127971, 105234, 45224, 108439, 13958, 905, 21694, 47963, 8395, 60355, 3380, 279, 65372, 123091, 52788, 58442, 7654, 25633, 41076, 22847, 3493, 74034, 57526, 115401, 96917, 124573, 82742, 98906, 17887, 1149, 17886, 31715, 20558, 27225, 131052, 115412, 60921, 16730, 36214, 100230, 77380, 280, 39992, 47219, 94363, 4902, 8966, 60725, 127553, 24939, 126530, 16838, 45806, 40447, 25843, 48847, 41686, 112086, 96110, 46697, 98373]\n"
     ]
    }
   ],
   "source": [
    "print(len(clustered))\n",
    "print(clustered[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q. How to recover kemans object from kmeans.cluster_centers_?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "clustering_utils.run_kmeans_clustering(data_points, k, init_center_points=center_points, knn_method=\"naive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Way to speed up codebook learning\n",
    "\n",
    "* [Annoy]() to assign centroids for all features\n",
    "* FLANN vs ANNOY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k means clustering with k=131072, knn_method:kdtree\n",
      "k-means clustering. start step:0\n",
      "num_same_centroid: 0\n",
      "k-means clustering. start step:1\n",
      "num_same_centroid: 1070631\n",
      "k-means clustering. start step:2\n",
      "num_same_centroid: 6305761\n",
      "k-means clustering. start step:3\n",
      "num_same_centroid: 8634853\n",
      "k-means clustering. start step:4\n",
      "num_same_centroid: 9524823\n",
      "k-means clustering. start step:5\n",
      "num_same_centroid: 9975007\n",
      "k-means clustering. start step:6\n",
      "num_same_centroid: 10253542\n",
      "k-means clustering. start step:7\n",
      "num_same_centroid: 10525599\n",
      "k-means clustering. start step:8\n",
      "num_same_centroid: 10715520\n",
      "k-means clustering. start step:9\n",
      "num_same_centroid: 10820592\n",
      "k-means clustering. start step:10\n",
      "num_same_centroid: 10881514\n",
      "k-means clustering. start step:11\n",
      "num_same_centroid: 10961578\n",
      "k-means clustering. start step:12\n",
      "num_same_centroid: 11051719\n",
      "k-means clustering. start step:13\n",
      "num_same_centroid: 11078194\n",
      "k-means clustering. start step:14\n",
      "num_same_centroid: 11135441\n",
      "k-means clustering. start step:15\n",
      "num_same_centroid: 11188490\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "center_points = clustering_utils.run_kmeans_clustering(data_points, k, max_iter=15, init_center_points=center_points, knn_method=\"kdtree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k means clustering with k=20000, knn_method:kdtree\n",
      "k-means clustering. start step:0\n",
      "num_same_centroid: 0\n",
      "k-means clustering. start step:1\n",
      "num_same_centroid: 74836\n",
      "k-means clustering. start step:2\n",
      "num_same_centroid: 78470\n",
      "k-means clustering. start step:3\n",
      "num_same_centroid: 79362\n",
      "k-means clustering. start step:4\n",
      "num_same_centroid: 79690\n",
      "k-means clustering. start step:5\n",
      "num_same_centroid: 79870\n",
      "k-means clustering. start step:6\n",
      "num_same_centroid: 79931\n",
      "k-means clustering. start step:7\n",
      "num_same_centroid: 79974\n",
      "k-means clustering. start step:8\n",
      "num_same_centroid: 79985\n",
      "k-means clustering. start step:9\n",
      "num_same_centroid: 79995\n",
      "k-means clustering. start step:10\n",
      "num_same_centroid: 80000\n",
      "cluster assignment is not changed. It means convergence!\n",
      "CPU times: user 47min 37s, sys: 4.91 s, total: 47min 42s\n",
      "Wall time: 7min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clustering_utils.run_kmeans_clustering(data_points.astype(float), k, init_center_points=center_points, knn_method=\"kdtree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k means clustering with k=20000, knn_method:kdtree\n",
      "k-means clustering. start step:0\n",
      "num_same_centroid: 0\n",
      "k-means clustering. start step:1\n",
      "num_same_centroid: 74767\n",
      "k-means clustering. start step:2\n",
      "num_same_centroid: 78475\n",
      "k-means clustering. start step:3\n",
      "num_same_centroid: 79350\n",
      "k-means clustering. start step:4\n",
      "num_same_centroid: 79707\n",
      "k-means clustering. start step:5\n",
      "num_same_centroid: 79870\n",
      "k-means clustering. start step:6\n",
      "num_same_centroid: 79921\n",
      "k-means clustering. start step:7\n",
      "num_same_centroid: 79954\n",
      "k-means clustering. start step:8\n",
      "num_same_centroid: 79980\n",
      "k-means clustering. start step:9\n",
      "num_same_centroid: 79993\n",
      "k-means clustering. start step:10\n",
      "num_same_centroid: 79994\n",
      "k-means clustering. start step:11\n",
      "num_same_centroid: 79997\n",
      "k-means clustering. start step:12\n",
      "num_same_centroid: 79999\n",
      "k-means clustering. start step:13\n",
      "num_same_centroid: 80000\n",
      "cluster assignment is not changed. It means convergence!\n",
      "CPU times: user 36min 40s, sys: 4.51 s, total: 36min 44s\n",
      "Wall time: 6min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clustering_utils.run_kmeans_clustering(data_points, k, init_center_points=center_points, knn_method=\"kdtree\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
