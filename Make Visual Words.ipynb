{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Visual Words\n",
    "\n",
    "## Rquirements\n",
    "\n",
    "k means clustering is memory intensive process. \n",
    "\n",
    "## Input\n",
    "\n",
    "* n-dim descriptors\n",
    "    * For making visual words, which image contains which descriptor is not important. We just do quantization.\n",
    "\n",
    "* Number of visual words\n",
    "\n",
    "## Process\n",
    "\n",
    "Build cluster.\n",
    "Say, we use k-means clustering. We determine number of clusters. \n",
    "Run k-means clustering. \n",
    "We can accelerate speed by using approximate nearest neighbor search such as kd-tree, but we lose accuracy. \n",
    "\n",
    "## Output\n",
    "\n",
    "index of visual words, and its cluster's center point. \n",
    "\n",
    "From a given feature, we can use the output information to determine most closest visual words. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read features from ./data/feature/feat_oxc1_hesaff_sift.bin\n",
      "16334970\n"
     ]
    }
   ],
   "source": [
    "# Load all features in the dataset. \n",
    "# Q. What if there is excatly same feature many times? do we add it into feature pool? \n",
    "# Q. Does many same feature point affect the result of k-menas clustering?\n",
    "\n",
    "# An image may contain several features, and we need to get all features. \n",
    "\n",
    "\n",
    "% load_ext autoreload\n",
    "% autoreload 2\n",
    "from utils.oxf5k_feature_reader import feature_reader\n",
    "\n",
    "all_features = feature_reader()\n",
    "print(len(all_features))\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "data_points: [[ 38  18   4 ...,  45  49  10]\n",
      " [110  50   4 ...,   4  44  43]\n",
      " [102  28   1 ...,  50 110   2]\n",
      " ..., \n",
      " [ 65  83  58 ...,   0   0   3]\n",
      " [ 34  64  70 ...,   1   1  13]\n",
      " [ 22   8  36 ...,  27  12  13]]\n",
      "data_points shape: (16334970, 128)\n",
      "data_points dtype: uint8\n",
      "k: 131072\n",
      "max_val: 255\n",
      "center_points: [[214  19 135 ..., 222  47 216]\n",
      " [ 69 182 213 ..., 137 149  36]\n",
      " [ 10   3 148 ..., 178 244 227]\n",
      " ..., \n",
      " [208 239 149 ...,  80  95  56]\n",
      " [ 37  68 222 ..., 168  44  84]\n",
      " [195 189 118 ...,  70 152  32]]\n",
      "center_points type: uint8\n"
     ]
    }
   ],
   "source": [
    "% load_ext autoreload\n",
    "% autoreload 2\n",
    "from utils import clustering_utils\n",
    "import numpy as np\n",
    "\n",
    "dim = 128\n",
    "\n",
    "# n = 80000\n",
    "# k = 20000\n",
    "# data_points = np.random.randint(256, size=(n, dim), dtype=np.uint8)\n",
    "\n",
    "data_points = np.array(all_features)\n",
    "k = 131072 # 2^17 = 131072\n",
    "\n",
    "# Timing History:\n",
    "# n = 80,000 k=20,000 => single k-d tree with FLANN, 13 step, 6 min \n",
    "\n",
    "\n",
    "print('data_points:', data_points)\n",
    "print('data_points shape:', data_points.shape)\n",
    "print('data_points dtype:', data_points.dtype)\n",
    "print('k:', k)\n",
    "\n",
    "max_val = 2**8-1 # 255. uint8 max. for SIFT descriptor\n",
    "print('max_val:', max_val)\n",
    "center_points = np.random.randint(max_val, size=(k, dim), dtype=np.uint8) \n",
    "# center_points = center_points.astype(float)\n",
    "print('center_points:', center_points)\n",
    "print('center_points type:', center_points.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encode with subsapce: 8\n",
      "shape of rand sample: (1000000, 128)\n",
      "save file path: pqencoder_1000k_random_sample_from_16M.pkl\n",
      "fitting encoder...\n",
      "done\n",
      "CPU times: user 29min 14s, sys: 1d 12h 18min 46s, total: 1d 12h 48min 1s\n",
      "Wall time: 57min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import random\n",
    "import pqkmeans\n",
    "import pickle\n",
    "\n",
    "# Train a PQ encoder.\n",
    "# Each vector is devided into 4 parts and each part is\n",
    "# encoded with log256 = 8 bit, resulting in a 32 bit PQ code.\n",
    "M=8\n",
    "print('encode with subsapce: {}'.format(M))\n",
    "encoder = pqkmeans.encoder.PQEncoder(num_subdim=M, Ks=256)\n",
    "# Q. do we have to use only subset? If we have train/test set split, use train for this. use test for query\n",
    "# encoder.fit(data_points[:1000])\n",
    "# time complexity for PQ: O(DL) where D is number of points, L is ???\n",
    "\n",
    "# num_to_select = 100000                     # set the number to select here. 100K for 16M\n",
    "\n",
    "num_to_select = 1000000 # Usually, people make 1M vocab in early 2010s\n",
    "# 16000000\n",
    "selected_index = np.random.choice(data_points.shape[0], num_to_select, replace=False)\n",
    "list_of_random_items = data_points[selected_index, :]\n",
    "print('shape of rand sample:', list_of_random_items.shape)\n",
    "\n",
    "save_file_path = 'pqencoder_{}k_random_sample_from_{}M.pkl'.format(num_to_select//1000, data_points.shape[0]//1000000)\n",
    "print(\"save file path:\", save_file_path)\n",
    "\n",
    "print(\"fitting encoder...\")\n",
    "encoder.fit(list_of_random_items)  # Use top 1M descriptor for training visual words codebook for oxford5k\n",
    "pickle.dump(encoder, open(save_file_path, 'wb'))\n",
    "print(\"done\")\n",
    "\n",
    "# Timing. codebook learning of 128d 4sub 100k took 3 min\n",
    "# Timing. codebook learning of 128d 4sub 1M took 30 min\n",
    "# Timing. codebook learning of 128d 8sub 1M took 58 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/16334970 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform whole set\n",
      "data_points_pqcode.shape:\n",
      "(16334970, 8)\n",
      "data_points_pqcode.nbytes:\n",
      "130679760 bytes\n",
      "data_points_pqcode.dtype:\n",
      "uint8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16334970/16334970 [03:55<00:00, 69291.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save file path: data_points_pqcode_with_1000k_codebook_8_sub.npy\n",
      "done\n",
      "CPU times: user 29min 25s, sys: 2h 7min 22s, total: 2h 36min 48s\n",
      "Wall time: 3min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('transform whole set')\n",
    "# Convert input vectors to 32-bit PQ codes, where each PQ code consists of four uint8.\n",
    "# You can train the encoder and transform the input vectors to PQ codes preliminary.\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "# # For big-data that cannot fit in memory, we can use generator\n",
    "pqcode_generator = encoder.transform_generator(data_points)\n",
    "N, _ = data_points.shape\n",
    "data_points_pqcode = np.empty([N, M], dtype=encoder.code_dtype)\n",
    "print(\"data_points_pqcode.shape:\\n{}\".format(data_points_pqcode.shape))\n",
    "print(\"data_points_pqcode.nbytes:\\n{} bytes\".format(data_points_pqcode.nbytes))\n",
    "print(\"data_points_pqcode.dtype:\\n{}\".format(data_points_pqcode.dtype))\n",
    "for n, code in enumerate(tqdm(pqcode_generator, total=N)):\n",
    "    data_points_pqcode[n, :] = code\n",
    "\n",
    "# For small data fit in memory, simply run this. \n",
    "# data_points_pqcode = encoder.transform(data_points)\n",
    "\n",
    "save_file_path = 'data_points_pqcode_with_{}k_codebook_{}_sub.npy'.format(num_to_select//1000, M)\n",
    "print(\"save file path:\", save_file_path)\n",
    "\n",
    "np.save(save_file_path, data_points_pqcode)\n",
    "print(\"done\")\n",
    "\n",
    "# Memory: With 32bit PQ code, 16M 128d descriptor takes 65339960 bytes = 62.3 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save file path: clustering_centers_numpy_16M_feature_1000k_coodebook_8_sub_131k_cluster.npy\n",
      "run k-means with k: 131072\n",
      "clustered len: 16334970\n",
      "CPU times: user 8d 12h 47min 28s, sys: 3min 34s, total: 8d 12h 51min 3s\n",
      "Wall time: 5h 10min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "save_file_path = 'clustering_centers_numpy_{}M_feature_{}k_coodebook_{}_sub_{}k_cluster.npy'.format(data_points.shape[0]//1000000, num_to_select//1000, M, k//1000)\n",
    "print(\"save file path:\", save_file_path)\n",
    "\n",
    "print('run k-means with k:', k)\n",
    "kmeans = pqkmeans.clustering.PQKMeans(encoder=encoder, k=k)\n",
    "clustered = kmeans.fit_predict(data_points_pqcode)\n",
    "\n",
    "print('clustered len:', len(clustered))\n",
    "\n",
    "clustering_centers_numpy = np.array(kmeans.cluster_centers_, dtype=encoder.code_dtype)  # Convert to np.array with the proper dtype\n",
    "np.save(save_file_path, clustering_centers_numpy)# Then, clustered[0] is the id of assigned center for the first input PQ code (X_pqcode[0]).\n",
    "\n",
    "# Timing: 16M features, 1M codebook with 4 subspaces, 131k cluster: 3h 39min\n",
    "# Timing: 16M features, 1M codebook with 8 subspaces, 131k cluster: 5h 10min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16334970\n",
      "[105031, 68426, 122039, 83601, 120423, 66520, 46672, 84148, 50034, 96736, 35462, 110366, 31576, 101061, 82376, 43444, 86647, 44124, 37170, 94394, 113728, 40562, 128085, 5021, 66410, 12129, 122921, 77915, 20192, 130256, 46154, 2664, 35714, 84516, 131045, 61253, 40068, 48473, 107411, 33163, 55187, 45048, 90057, 67174, 69707, 55895, 78043, 87026, 123114, 119128, 16212, 122787, 7118, 8133, 34621, 6291, 44602, 79164, 3581, 102205, 51221, 35374, 19250, 16128, 24437, 52062, 108034, 45757, 25169, 80047, 114605, 20558, 27225, 110869, 110752, 103082, 99896, 49008, 99177, 60879, 31801, 68656, 33775, 94363, 87216, 115390, 9619, 41610, 119355, 36939, 64928, 85471, 86170, 32955, 45586, 60633, 14970, 92233, 2007, 105419]\n"
     ]
    }
   ],
   "source": [
    "print(len(clustered))\n",
    "print(clustered[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------- Below codes are legacy ------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "clustering_utils.run_kmeans_clustering(data_points, k, init_center_points=center_points, knn_method=\"naive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Way to speed up codebook learning\n",
    "\n",
    "* [Annoy]() to assign centroids for all features\n",
    "* FLANN vs ANNOY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k means clustering with k=131072, knn_method:kdtree\n",
      "k-means clustering. start step:0\n",
      "num_same_centroid: 0\n",
      "k-means clustering. start step:1\n",
      "num_same_centroid: 1070631\n",
      "k-means clustering. start step:2\n",
      "num_same_centroid: 6305761\n",
      "k-means clustering. start step:3\n",
      "num_same_centroid: 8634853\n",
      "k-means clustering. start step:4\n",
      "num_same_centroid: 9524823\n",
      "k-means clustering. start step:5\n",
      "num_same_centroid: 9975007\n",
      "k-means clustering. start step:6\n",
      "num_same_centroid: 10253542\n",
      "k-means clustering. start step:7\n",
      "num_same_centroid: 10525599\n",
      "k-means clustering. start step:8\n",
      "num_same_centroid: 10715520\n",
      "k-means clustering. start step:9\n",
      "num_same_centroid: 10820592\n",
      "k-means clustering. start step:10\n",
      "num_same_centroid: 10881514\n",
      "k-means clustering. start step:11\n",
      "num_same_centroid: 10961578\n",
      "k-means clustering. start step:12\n",
      "num_same_centroid: 11051719\n",
      "k-means clustering. start step:13\n",
      "num_same_centroid: 11078194\n",
      "k-means clustering. start step:14\n",
      "num_same_centroid: 11135441\n",
      "k-means clustering. start step:15\n",
      "num_same_centroid: 11188490\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "center_points = clustering_utils.run_kmeans_clustering(data_points, k, max_iter=15, init_center_points=center_points, knn_method=\"kdtree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k means clustering with k=20000, knn_method:kdtree\n",
      "k-means clustering. start step:0\n",
      "num_same_centroid: 0\n",
      "k-means clustering. start step:1\n",
      "num_same_centroid: 74836\n",
      "k-means clustering. start step:2\n",
      "num_same_centroid: 78470\n",
      "k-means clustering. start step:3\n",
      "num_same_centroid: 79362\n",
      "k-means clustering. start step:4\n",
      "num_same_centroid: 79690\n",
      "k-means clustering. start step:5\n",
      "num_same_centroid: 79870\n",
      "k-means clustering. start step:6\n",
      "num_same_centroid: 79931\n",
      "k-means clustering. start step:7\n",
      "num_same_centroid: 79974\n",
      "k-means clustering. start step:8\n",
      "num_same_centroid: 79985\n",
      "k-means clustering. start step:9\n",
      "num_same_centroid: 79995\n",
      "k-means clustering. start step:10\n",
      "num_same_centroid: 80000\n",
      "cluster assignment is not changed. It means convergence!\n",
      "CPU times: user 47min 37s, sys: 4.91 s, total: 47min 42s\n",
      "Wall time: 7min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clustering_utils.run_kmeans_clustering(data_points.astype(float), k, init_center_points=center_points, knn_method=\"kdtree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k means clustering with k=20000, knn_method:kdtree\n",
      "k-means clustering. start step:0\n",
      "num_same_centroid: 0\n",
      "k-means clustering. start step:1\n",
      "num_same_centroid: 74767\n",
      "k-means clustering. start step:2\n",
      "num_same_centroid: 78475\n",
      "k-means clustering. start step:3\n",
      "num_same_centroid: 79350\n",
      "k-means clustering. start step:4\n",
      "num_same_centroid: 79707\n",
      "k-means clustering. start step:5\n",
      "num_same_centroid: 79870\n",
      "k-means clustering. start step:6\n",
      "num_same_centroid: 79921\n",
      "k-means clustering. start step:7\n",
      "num_same_centroid: 79954\n",
      "k-means clustering. start step:8\n",
      "num_same_centroid: 79980\n",
      "k-means clustering. start step:9\n",
      "num_same_centroid: 79993\n",
      "k-means clustering. start step:10\n",
      "num_same_centroid: 79994\n",
      "k-means clustering. start step:11\n",
      "num_same_centroid: 79997\n",
      "k-means clustering. start step:12\n",
      "num_same_centroid: 79999\n",
      "k-means clustering. start step:13\n",
      "num_same_centroid: 80000\n",
      "cluster assignment is not changed. It means convergence!\n",
      "CPU times: user 36min 40s, sys: 4.51 s, total: 36min 44s\n",
      "Wall time: 6min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clustering_utils.run_kmeans_clustering(data_points, k, init_center_points=center_points, knn_method=\"kdtree\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
