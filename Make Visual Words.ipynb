{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Visual Words\n",
    "\n",
    "## Rquirements\n",
    "\n",
    "k means clustering is memory intensive process. \n",
    "\n",
    "## Input\n",
    "\n",
    "* n-dim descriptors\n",
    "    * For making visual words, which image contains which descriptor is not important. We just do quantization.\n",
    "\n",
    "* Number of visual words\n",
    "\n",
    "## Process\n",
    "\n",
    "Build cluster.\n",
    "Say, we use k-means clustering. We determine number of clusters. \n",
    "Run k-means clustering. \n",
    "We can accelerate speed by using approximate nearest neighbor search such as kd-tree, but we lose accuracy. \n",
    "\n",
    "## Output\n",
    "\n",
    "index of visual words, and its cluster's center point. \n",
    "\n",
    "From a given feature, we can use the output information to determine most closest visual words. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16334970\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "233"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load all features in the dataset. \n",
    "# Q. What if there is excatly same feature many times? do we add it into feature pool? \n",
    "# Q. Does many same feature point affect the result of k-menas clustering?\n",
    "\n",
    "# An image may contain several features, and we need to get all features. \n",
    "\n",
    "\n",
    "% load_ext autoreload\n",
    "% autoreload 2\n",
    "# from utils import oxf5k_feature_reader as feature_reader\n",
    "\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "def feature_reader(feature_bin_path=\"./data/feature/feat_oxc1_hesaff_sift.bin\"):\n",
    "    \"\"\"\n",
    "    This method reads official oxf5k descriptor.\n",
    "    \n",
    "    binary file contains 128byte sift descriptor. It is 128-d integer vector. \n",
    "    \"\"\"\n",
    "        \n",
    "    features = []    \n",
    "    # Read feature from bin file, and make tuple format. \n",
    "    with open(feature_bin_path, \"rb\") as f:\n",
    "        # Read 128 byte        \n",
    "        raw_binary = f.read(128)\n",
    "        while len(raw_binary) == 128:\n",
    "            dt = np.dtype(np.uint8)\n",
    "            # dt = dt.newbyteorder('>')\n",
    "            descriptor = np.frombuffer(raw_binary, dtype=dt)\n",
    "            features.append(descriptor)\n",
    "            raw_binary = f.read(128)\n",
    "            \n",
    "    return features\n",
    "\n",
    "\n",
    "def load_features_from_dataset():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "all_features = feature_reader()\n",
    "print(len(all_features))\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "data_points: [[ 38  18   4 ...,  45  49  10]\n",
      " [110  50   4 ...,   4  44  43]\n",
      " [102  28   1 ...,  50 110   2]\n",
      " ..., \n",
      " [ 65  83  58 ...,   0   0   3]\n",
      " [ 34  64  70 ...,   1   1  13]\n",
      " [ 22   8  36 ...,  27  12  13]]\n",
      "data_points shape: (16334970, 128)\n",
      "data_points dtype: uint8\n",
      "k: 131072\n",
      "max_val: 255\n",
      "center_points: [[214 200 217 ...,  78  78 195]\n",
      " [151  23 150 ..., 167  57 240]\n",
      " [237 189  49 ..., 107 213 182]\n",
      " ..., \n",
      " [ 11 188  40 ..., 155  39 167]\n",
      " [ 11 197 223 ...,  40 135  77]\n",
      " [ 68 251 116 ..., 179  21  89]]\n",
      "center_points type: uint8\n"
     ]
    }
   ],
   "source": [
    "% load_ext autoreload\n",
    "% autoreload 2\n",
    "from utils import clustering_utils\n",
    "import numpy as np\n",
    "\n",
    "dim = 128\n",
    "\n",
    "# n = 80000\n",
    "# k = 20000\n",
    "# data_points = np.random.randint(256, size=(n, dim), dtype=np.uint8)\n",
    "\n",
    "data_points = np.array(all_features)\n",
    "k = 131072 # 2^17 = 131072\n",
    "\n",
    "# Timing History:\n",
    "# n = 80,000 k=20,000 => single k-d tree with FLANN, 13 step, 6 min \n",
    "\n",
    "\n",
    "print('data_points:', data_points)\n",
    "print('data_points shape:', data_points.shape)\n",
    "print('data_points dtype:', data_points.dtype)\n",
    "print('k:', k)\n",
    "\n",
    "max_val = 2**8-1 # 255. uint8 max. for SIFT descriptor\n",
    "print('max_val:', max_val)\n",
    "center_points = np.random.randint(max_val, size=(k, dim), dtype=np.uint8) \n",
    "# center_points = center_points.astype(float)\n",
    "print('center_points:', center_points)\n",
    "print('center_points type:', center_points.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encode with subset\n",
      "CPU times: user 18min 45s, sys: 18h 12min 45s, total: 18h 31min 31s\n",
      "Wall time: 29min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pqkmeans\n",
    "import pickle\n",
    "\n",
    "# Train a PQ encoder.\n",
    "# Each vector is devided into 4 parts and each part is\n",
    "# encoded with log256 = 8 bit, resulting in a 32 bit PQ code.\n",
    "print('encode with subset')\n",
    "M=4\n",
    "encoder = pqkmeans.encoder.PQEncoder(num_subdim=M, Ks=256)\n",
    "# Q. do we have to use only subset? If we have train/test set split, use train for this. use test for query\n",
    "# encoder.fit(data_points[:1000])\n",
    "# time complexity for PQ: O(DL) where D is number of points, L is ???\n",
    "encoder.fit(data_points[:1000000])  # Use top 1M descriptor for training visual words codebook for oxford5k\n",
    "pickle.dump(encoder, open('encoder.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/16334970 [00:00<514:13:56,  8.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform whole set\n",
      "data_points_pqcode.shape:\n",
      "(16334970, 4)\n",
      "data_points_pqcode.nbytes:\n",
      "65339880 bytes\n",
      "data_points_pqcode.dtype:\n",
      "uint8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16334970/16334970 [02:54<00:00, 93489.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22min 15s, sys: 1h 34min 8s, total: 1h 56min 23s\n",
      "Wall time: 2min 54s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('transform whole set')\n",
    "# Convert input vectors to 32-bit PQ codes, where each PQ code consists of four uint8.\n",
    "# You can train the encoder and transform the input vectors to PQ codes preliminary.\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "# # For big-data that cannot fit in memory, we can use generator\n",
    "pqcode_generator = encoder.transform_generator(data_points)\n",
    "N, _ = data_points.shape\n",
    "data_points_pqcode = np.empty([N, M], dtype=encoder.code_dtype)\n",
    "print(\"data_points_pqcode.shape:\\n{}\".format(data_points_pqcode.shape))\n",
    "print(\"data_points_pqcode.nbytes:\\n{} bytes\".format(data_points_pqcode.nbytes))\n",
    "print(\"data_points_pqcode.dtype:\\n{}\".format(data_points_pqcode.dtype))\n",
    "for n, code in enumerate(tqdm(pqcode_generator, total=N)):\n",
    "    data_points_pqcode[n, :] = code\n",
    "\n",
    "# For small data fit in memory, simply run this. \n",
    "# data_points_pqcode = encoder.transform(data_points)\n",
    "\n",
    "np.save('data_points_pqcode.npy', data_points_pqcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run k-means with k: 131072\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't pickle _pqkmeans.PQKMeans objects",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't pickle _pqkmeans.PQKMeans objects"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('run k-means with k:', k)\n",
    "# Run clustering with k=5 clusters.\n",
    "kmeans = pqkmeans.clustering.PQKMeans(encoder=encoder, k=k)\n",
    "clustered = kmeans.fit_predict(data_points_pqcode)\n",
    "\n",
    "print('clustered len:', len(clustered))\n",
    "\n",
    "clustering_centers_numpy = np.array(kmeans.cluster_centers_, dtype=encoder.code_dtype)  # Convert to np.array with the proper dtype\n",
    "np.save('clustering_centers_numpy.npy', clustering_centers_numpy)# Then, clustered[0] is the id of assigned center for the first input PQ code (X_pqcode[0])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16334970\n",
      "[63282, 50010, 92089, 81189, 21063, 66520, 46672, 76169, 49071, 67414, 48013, 29133, 26552, 119182, 82376, 65436, 44761, 34527, 65975, 13034, 37923, 9074, 62471, 77480, 112837, 48880, 48533, 120092, 18133, 5037, 42087, 88979, 80304, 87754, 131045, 16786, 115494, 106030, 74411, 58582, 121602, 45224, 76870, 73495, 10629, 69107, 78043, 113028, 25101, 5110, 7054, 85535, 87005, 42624, 113283, 20710, 65208, 16873, 5370, 33726, 55063, 120055, 1947, 24614, 39952, 82136, 108034, 67621, 18490, 118866, 25119, 20558, 52050, 50779, 27128, 5015, 19313, 91604, 92100, 60879, 280, 8280, 82264, 94363, 90620, 86251, 109833, 77385, 24939, 37156, 81603, 81470, 11455, 75787, 115062, 2887, 90975, 40920, 46697, 94815]\n"
     ]
    }
   ],
   "source": [
    "clustering_centers_numpy = np.array(kmeans.cluster_centers_, dtype=encoder.code_dtype)  # Convert to np.array with the proper dtype\n",
    "np.save('clustering_centers_numpy.npy', clustering_centers_numpy)\n",
    "\n",
    "print(len(clustered))\n",
    "print(clustered[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q. How to recover kemans object from kmeans.cluster_centers_?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "clustering_utils.run_kmeans_clustering(data_points, k, init_center_points=center_points, knn_method=\"naive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Way to speed up codebook learning\n",
    "\n",
    "* [Annoy]() to assign centroids for all features\n",
    "* FLANN vs ANNOY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k means clustering with k=131072, knn_method:kdtree\n",
      "k-means clustering. start step:0\n",
      "num_same_centroid: 0\n",
      "k-means clustering. start step:1\n",
      "num_same_centroid: 1070631\n",
      "k-means clustering. start step:2\n",
      "num_same_centroid: 6305761\n",
      "k-means clustering. start step:3\n",
      "num_same_centroid: 8634853\n",
      "k-means clustering. start step:4\n",
      "num_same_centroid: 9524823\n",
      "k-means clustering. start step:5\n",
      "num_same_centroid: 9975007\n",
      "k-means clustering. start step:6\n",
      "num_same_centroid: 10253542\n",
      "k-means clustering. start step:7\n",
      "num_same_centroid: 10525599\n",
      "k-means clustering. start step:8\n",
      "num_same_centroid: 10715520\n",
      "k-means clustering. start step:9\n",
      "num_same_centroid: 10820592\n",
      "k-means clustering. start step:10\n",
      "num_same_centroid: 10881514\n",
      "k-means clustering. start step:11\n",
      "num_same_centroid: 10961578\n",
      "k-means clustering. start step:12\n",
      "num_same_centroid: 11051719\n",
      "k-means clustering. start step:13\n",
      "num_same_centroid: 11078194\n",
      "k-means clustering. start step:14\n",
      "num_same_centroid: 11135441\n",
      "k-means clustering. start step:15\n",
      "num_same_centroid: 11188490\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "center_points = clustering_utils.run_kmeans_clustering(data_points, k, max_iter=15, init_center_points=center_points, knn_method=\"kdtree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k means clustering with k=20000, knn_method:kdtree\n",
      "k-means clustering. start step:0\n",
      "num_same_centroid: 0\n",
      "k-means clustering. start step:1\n",
      "num_same_centroid: 74836\n",
      "k-means clustering. start step:2\n",
      "num_same_centroid: 78470\n",
      "k-means clustering. start step:3\n",
      "num_same_centroid: 79362\n",
      "k-means clustering. start step:4\n",
      "num_same_centroid: 79690\n",
      "k-means clustering. start step:5\n",
      "num_same_centroid: 79870\n",
      "k-means clustering. start step:6\n",
      "num_same_centroid: 79931\n",
      "k-means clustering. start step:7\n",
      "num_same_centroid: 79974\n",
      "k-means clustering. start step:8\n",
      "num_same_centroid: 79985\n",
      "k-means clustering. start step:9\n",
      "num_same_centroid: 79995\n",
      "k-means clustering. start step:10\n",
      "num_same_centroid: 80000\n",
      "cluster assignment is not changed. It means convergence!\n",
      "CPU times: user 47min 37s, sys: 4.91 s, total: 47min 42s\n",
      "Wall time: 7min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clustering_utils.run_kmeans_clustering(data_points.astype(float), k, init_center_points=center_points, knn_method=\"kdtree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k means clustering with k=20000, knn_method:kdtree\n",
      "k-means clustering. start step:0\n",
      "num_same_centroid: 0\n",
      "k-means clustering. start step:1\n",
      "num_same_centroid: 74767\n",
      "k-means clustering. start step:2\n",
      "num_same_centroid: 78475\n",
      "k-means clustering. start step:3\n",
      "num_same_centroid: 79350\n",
      "k-means clustering. start step:4\n",
      "num_same_centroid: 79707\n",
      "k-means clustering. start step:5\n",
      "num_same_centroid: 79870\n",
      "k-means clustering. start step:6\n",
      "num_same_centroid: 79921\n",
      "k-means clustering. start step:7\n",
      "num_same_centroid: 79954\n",
      "k-means clustering. start step:8\n",
      "num_same_centroid: 79980\n",
      "k-means clustering. start step:9\n",
      "num_same_centroid: 79993\n",
      "k-means clustering. start step:10\n",
      "num_same_centroid: 79994\n",
      "k-means clustering. start step:11\n",
      "num_same_centroid: 79997\n",
      "k-means clustering. start step:12\n",
      "num_same_centroid: 79999\n",
      "k-means clustering. start step:13\n",
      "num_same_centroid: 80000\n",
      "cluster assignment is not changed. It means convergence!\n",
      "CPU times: user 36min 40s, sys: 4.51 s, total: 36min 44s\n",
      "Wall time: 6min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clustering_utils.run_kmeans_clustering(data_points, k, init_center_points=center_points, knn_method=\"kdtree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-88220ec2899e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mvisual_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_visual_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkmeans_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'images' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_visual_words(kmeans_info, features):\n",
    "    \"\"\"\n",
    "    convert features to list of visual word index. \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "for image in images:\n",
    "    visual_words = get_visual_words(kmeans_info, features)\n",
    "    \n",
    "\n",
    "def save_visual_words_to_file(visual_words, output_dir):\n",
    "    pass\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
