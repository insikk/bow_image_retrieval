{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# minHash Image Clustering (MHIC) algorithm (Seed Generation Only)\n",
    "\n",
    "Implementation based on \n",
    "* [Large-Scale Discovery of Spatially Related Images](ieeexplore.ieee.org/iel5/34/4359286/05235143.pdf) by Ondrej Chum and Jiri Matas\n",
    "* [Scalable Near Identical Image and Shot Detection - Microsoft](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/civr2007.pdf) by Ondrej Chum, James Philbin, Michael Isard, Andrew Zisserman\n",
    "\n",
    "## Purpose\n",
    "\n",
    "If we see a similar image cluster as a connected compoenent, images are vertex. \n",
    "We have to find edges to get image cluster. minHash can be used to find subset of the edges quickly. \n",
    "\n",
    "Afterward, you may use image retrieval system to complete the connected component. \n",
    "\n",
    "\n",
    "## Requirements\n",
    "\n",
    "* Visual words index list for each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# bow_dict_save_path = 'bow_dict_word_oxc1_hesaff_sift_16M_1M_pretrained.pkl'\n",
    "# vocab_size=1000000\n",
    "\n",
    "# bow_dict_save_path = 'bow_dict_word_oxc1_hesaff_sift_16M_100k_handmade.pkl'\n",
    "# vocab_size = 2**17\n",
    "\n",
    "# bow_dict_save_path = 'bow_dict_word_oxc1_hesaff_sift_16M_1M_4_sub_handmade.pkl'\n",
    "# vocab_size = 2**17\n",
    "\n",
    "bow_dict_save_path = 'bow_dict_word_oxc1_hesaff_sift_16M_1M_8_sub_handmade.pkl'\n",
    "vocab_size = 2**17\n",
    "\n",
    "with open(bow_dict_save_path, 'rb') as f:\n",
    "    # key: image_name, value: list of visual word index\n",
    "    bow_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from utils.bow_utils import get_idf_word_weights\n",
    "\n",
    "word_weights = get_idf_word_weights(bow_dict, vocab_size=vocab_size)\n",
    "print(word_weights[:10]) # inverse document frequency. It is importance of the words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# from utils.minhash_utils import VisualMinHashWithDataSketch\n",
    "from utils.minhash_utils import SketchCollisionTester, VisualMinHashWithLookupTable\n",
    "from utils.minhash_utils import get_collision_pairs\n",
    "\n",
    "def print_similar_paris_statistics(similar_pairs, th_sim_min, th_sim_max):\n",
    "    similar_pairs = list(similar_pairs)\n",
    "    print('len similar_pairs:', len(similar_pairs))\n",
    "    similar_pairs.sort(key=lambda x: x[1])\n",
    "    print('similar_pairs[:5]:', similar_pairs[:5])\n",
    "\n",
    "    count_irr = 0\n",
    "    count_sim = 0\n",
    "    count_dup = 0\n",
    "\n",
    "    for image_cluster, score in similar_pairs:\n",
    "        if score < THRESHOLD_DATAMINING_SIMILARITY_MIN:\n",
    "            count_irr += 1            \n",
    "        elif score >= THRESHOLD_DATAMINING_SIMILARITY_MIN and score < THRESHOLD_DATAMINING_SIMILARITY_MAX:\n",
    "            count_sim += 1\n",
    "        else:\n",
    "            count_dup += 1\n",
    "    print(\"count (irr, sim, dup) : ({}, {}, {})\".format(count_irr, count_sim, count_dup))\n",
    "    \n",
    "\n",
    "collision_tester = SketchCollisionTester(minHash_param_k=512)        \n",
    "# vmh = VisualMinHashWithDataSketch(minHash_hash_num=512, minHash_param_k=512, minHash_param_s=3)\n",
    "vmh = VisualMinHashWithLookupTable(minHash_hash_num=512, vocab_size=vocab_size, word_weights=word_weights, minHash_param_k=512, minHash_param_s=3)\n",
    "\n",
    "similar_pairs = get_collision_pairs(bow_dict, vmh, collision_tester)\n",
    "\n",
    "# For datamining purpose, we want to get less simlar but the same scene. \n",
    "# So we are interested in similiarity in [THRESHOLD_DATAMINING_SIMILARITY_MIN, THRESHOLD_DATAMINING_SIMILARITY_MAX]\n",
    "# See Large-Scale Discovery of Spatially Related Images. Sec 3.2 for THRESHOLD_DATAMINING_SIMILARITY_MIN\n",
    "# See Scaleable Near Identical Image and Shot Detection. Sec 4.3 for THRESHOLD_DATAMINING_SIMILARITY_MAX\n",
    "THRESHOLD_DATAMINING_SIMILARITY_MIN = 0.045 \n",
    "THRESHOLD_DATAMINING_SIMILARITY_MAX = 0.35\n",
    "\n",
    "print_similar_paris_statistics(similar_pairs, THRESHOLD_DATAMINING_SIMILARITY_MIN, THRESHOLD_DATAMINING_SIMILARITY_MAX)\n",
    "\n",
    "# Timing: 5062 images took 2 min 30 sec. \n",
    "\n",
    "# History: \n",
    "# Oxf5k, 16M features, 1M cluster(visual_words), count (irr, sim, dup) : \n",
    "# Oxf5k, 16M features, codebook train size 100k with 4 subspaces, 2^17 cluster(visual_words), count (irr, sim, dup) : (15348, 1141, 2)\n",
    "# Oxf5k, 16M features, codebook train size 1M with 4 subspaces, 2^17 cluster(visual_words), count (irr, sim, dup) : (15587, 1155, 2)\n",
    "# Oxf5k, 16M features, codebook train size 1M with 8 subspaces, 2^17 cluster(visual_words), count (irr, sim, dup) : (12775, 724, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from multiprocessing import Pool\n",
    "from utils.geo_verif_utils import get_ransac_inlier, draw_ransac\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import os \n",
    "\n",
    "def get_keypoints(image_name):\n",
    "    # Oxford 5k dataset provides already converted visual words. We could use this one    \n",
    "    oxf5k_visualword_dir = './data/word_oxc1_hesaff_sift_16M_1M'\n",
    "    filepath = os.path.join(oxf5k_visualword_dir, image_name + \".txt\")\n",
    "    kp = []\n",
    "    with open(filepath) as f:\n",
    "        lines = list(map(lambda x: x.strip(), f.readlines()[2:])) # ignore first two lines        \n",
    "        for l in lines:\n",
    "            val = l.split(\" \")\n",
    "            visual_word_index = int(val[0])-1 # This data use 1 to 1,000,000. convert to zero-based so 0 to 999,999  \n",
    "            x = float(val[1])\n",
    "            y = float(val[2])\n",
    "            a = float(val[3])\n",
    "            b = float(val[4])\n",
    "            c = float(val[5])\n",
    "            # TODO: generate ellipse shaped key point\n",
    "            # Refer: https://math.stackexchange.com/questions/1447730/drawing-ellipse-from-eigenvalue-eigenvector\n",
    "            # Refer: http://www.robots.ox.ac.uk/~vgg/research/affine/det_eval_files/display_features.m\n",
    "            # Refer: http://www.robots.ox.ac.uk/~vgg/research/affine/detectors.html\n",
    "            key_point = cv2.KeyPoint(x, y, 1)\n",
    "            kp.append(key_point)\n",
    "    return kp\n",
    "    \n",
    "\n",
    "image_descriptor_dict_path = 'image_descriptor_dict_oxc1_hesaff_sift_16M.pkl'\n",
    "with open(image_descriptor_dict_path, 'rb') as f:\n",
    "    # key: image_name, value: 2d numpy array of shape (num_descriptor, dim_descriptor)\n",
    "    image_descriptor_dict = pickle.load(f) \n",
    "    \n",
    "IMAGE_DIR = \"./data/oxford/oxford5k/images\"\n",
    "\n",
    "    \n",
    "def show_image_pair_ransac(image_dir, image_names, kp1, kp2, des1, des2):\n",
    "    \"\"\"\n",
    "    show image cluster for oxford 5k dataset\n",
    "    \"\"\"\n",
    "    # Visualize images assigned to this cluster    \n",
    "    from PIL import Image\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    imgs = []    \n",
    "    for image_name in image_names:\n",
    "        image_name = image_name.replace(\"oxc1_\", \"\") + \".jpg\"\n",
    "        image_path = os.path.join(image_dir, image_name)\n",
    "        img = cv2.imread(image_path)\n",
    "        imgs.append(img)\n",
    "        \n",
    "    return draw_ransac(imgs[0], imgs[1], kp1, kp2, des1, des2, False, 'BRUTE_FORCE', None)\n",
    "    \n",
    "\n",
    "def parallel_task(val):\n",
    "    image_cluster, score = val\n",
    "    kp1 = get_keypoints(image_cluster[0])\n",
    "    des1 = image_descriptor_dict[image_cluster[0]]\n",
    "    kp2 = get_keypoints(image_cluster[1])\n",
    "    des2 = image_descriptor_dict[image_cluster[1]]\n",
    "    num_inlier = get_ransac_inlier(kp1, kp2, des1, des2)\n",
    "    # num_inlier = show_image_pair_ransac(IMAGE_DIR, image_cluster, kp1, kp2, des1, des2)\n",
    "    return (image_cluster, score, num_inlier)\n",
    "    \n",
    "def filter_keep_similar_only(score):\n",
    "    return score >= THRESHOLD_DATAMINING_SIMILARITY_MIN and score < THRESHOLD_DATAMINING_SIMILARITY_MAX\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pool = Pool(processes=10)\n",
    "    cluster_seed = []    \n",
    "    similar_pairs = list(filter(lambda x: filter_keep_similar_only(x[1]), similar_pairs))\n",
    "    for result in tqdm(pool.imap_unordered(parallel_task, similar_pairs), total=len(similar_pairs)):\n",
    "        if result is not None:\n",
    "            cluster_seed.append(result)\n",
    "    print(cluster_seed[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def show_image_cluster(image_dir, image_names):\n",
    "    \"\"\"\n",
    "    show image cluster for oxford 5k dataset\n",
    "    \"\"\"\n",
    "    # Visualize images assigned to this cluster    \n",
    "    from PIL import Image\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    imgs = []    \n",
    "    for image_name in image_names:\n",
    "        image_name = image_name.replace(\"oxc1_\", \"\") + \".jpg\"\n",
    "        image_path = os.path.join(image_dir, image_name)\n",
    "        img = Image.open(image_path)\n",
    "        imgs.append(img)            \n",
    "        \n",
    "    cols = 5\n",
    "    imgs = imgs[:cols]\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    for i, img in enumerate(imgs):\n",
    "        plt.subplot(1, cols, i + 1)\n",
    "        plt.imshow(img)\n",
    "    plt.show()\n",
    "    \n",
    "print(similar_pairs[:5])\n",
    "    \n",
    "# IMAGE_DIR = \"./data/oxford5k_images\"\n",
    "IMAGE_DIR = \"./data/oxford/oxford5k/images\"\n",
    "similar_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "for image_cluster, score, num_inlier in similar_pairs:\n",
    "    print(\"pair: {}, score: {}\".format(image_cluster, score, num_inlier))\n",
    "    show_image_cluster(IMAGE_DIR, image_cluster)\n",
    "    print('\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def show_image_cluster(image_dir, image_names):\n",
    "    \"\"\"\n",
    "    show image cluster for oxford 5k dataset\n",
    "    \"\"\"\n",
    "    # Visualize images assigned to this cluster    \n",
    "    from PIL import Image\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    imgs = []    \n",
    "    for image_name in image_names:\n",
    "        image_name = image_name.replace(\"oxc1_\", \"\") + \".jpg\"\n",
    "        image_path = os.path.join(image_dir, image_name)\n",
    "        img = Image.open(image_path)\n",
    "        imgs.append(img)            \n",
    "        \n",
    "    cols = 5\n",
    "    imgs = imgs[:cols]\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    for i, img in enumerate(imgs):\n",
    "        plt.subplot(1, cols, i + 1)\n",
    "        plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "# IMAGE_DIR = \"./data/oxford5k_images\"\n",
    "IMAGE_DIR = \"./data/oxford/oxford5k/images\"\n",
    "sample_count = 10\n",
    "print(\"Sampling from irrelevant images.\")\n",
    "target_seq = similar_pairs[:count_irr]\n",
    "k = min(sample_count, len(target_seq))\n",
    "for image_cluster, score in random.sample(target_seq, k):\n",
    "    print(\"pair: {}, score: {}\".format(image_cluster, score))\n",
    "    show_image_cluster(IMAGE_DIR, image_cluster)\n",
    "    print('\\n')    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"Sampling from similar images.\")\n",
    "target_seq = similar_pairs[count_irr:count_irr+count_sim]\n",
    "k = min(sample_count, len(target_seq))\n",
    "target_seq.sort(key=lambda x: x[1], reverse=True)\n",
    "for image_cluster, score in target_seq:\n",
    "    print(\"pair: {}, score: {}\".format(image_cluster, score))\n",
    "    show_image_cluster(IMAGE_DIR, image_cluster)\n",
    "    print('\\n')    \n",
    "    \n",
    "print(\"Sampling from near-duplicates images.\")\n",
    "target_seq = similar_pairs[count_irr+count_sim:]\n",
    "target_seq.sort(key=lambda x: x[1], reverse=True)\n",
    "k = min(sample_count, len(target_seq))\n",
    "for image_cluster, score in random.sample(target_seq, k):\n",
    "    print(\"pair: {}, score: {}\".format(image_cluster, score))\n",
    "    show_image_cluster(IMAGE_DIR, image_cluster)\n",
    "    print('\\n')    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
